
import os
import json
import re
from typing import List, Dict
from openai import OpenAI
from htp_vec_store import htp_vector_store

class ProcessAwareResponseGenerator:
    """
    Handles fetching process details and logs related to a mortgage inquiry.
    """

    def __init__(self, api_key: str ,  log_dir: str , milvus_uri :str):
        """
        Initializes the handler with file paths and API credentials.

        Args:
            process_file (str): Path to the process JSON file.
            log_file (str): Path to the log file.
            api_key (str): API key for the OpenAI model.
        """
        self.log_dir = log_dir
        self.llm_client = OpenAI(api_key=api_key)
        self.milvus_uri=milvus_uri
        self.htp_store=htp_vector_store(milvus_uri=self.milvus_uri,htp_dir="./workflows",load_local=False)

    def load_htps(self , user_query) -> Dict:
        """
        Loads the mortgage htp data from milvus collection.

        Returns:
            Dict: The loaded htp details.
        """
        #self.htp_store.add_htps("./new_workflows","./workflows")
        htp_data=self.htp_store.hybrid_search(
            user_query,
            depth=3,
            top_k=3
        )
        return htp_data

    def extract_relevant_logs(self,htp_data) -> List[str]:
        """
        Extracts relevant logs from log files corresponding to HTP file paths based on the pattern '## CODE LOG ##'.

        Returns:
            List[str]: List of extracted log messages.
        """
        relevant_logs = []
        pattern = r"## CODE LOG ##\s*(.+)"

        for item in htp_data:
            path = item.get("file_path", "")  # e.g., "workflow_64.json"
            print("Workflow:",path)
            match = re.search(r"workflow_(\d+)\.json", path)
            if match:
                num = match.group(1)
                log_file_name = f"annie-core-backend_{num}.log"
                log_file_path = os.path.join(self.log_dir, log_file_name)
                print(log_file_path)
                if os.path.exists(log_file_path):
                    try:
                        with open(log_file_path, "r", encoding="utf-8") as file:
                            for line in file:
                                match = re.search(pattern, line)
                                if match:
                                    relevant_logs.append(match.group(1).strip())
                    except Exception:
                        continue  # Ignore file read errors silently

        return relevant_logs
    
    def create_prompt(self, user_query: str, process_data: Dict, logs: List[str]) -> str:
        """
        Creates an optimal prompt for the LLM.

        Args:
            user_query (str): The userâ€™s question about their mortgage process.
            process_data (Dict): The process details from the JSON file.
            logs (List[str]): The filtered relevant logs.

        Returns:
            str: The constructed prompt.
        """
        prompt=f"""
            You are an expert mortgage process assistant. Your task is to provide detailed explainations to user query using the process details and system logs.
            User Query: '{user_query}'\n
            
            process details:{json.dumps(process_data, indent=2)}\n
            
            Relevant system logs:\n{'; '.join(logs)}\n

            ANALYSIS INSTRUCTIONS:
            -Identify the key process stages from the workflow data
            -Correlate log entries with each workflow step
            -Explain what happened in natural language 

            RESPONSE GUIDELINESS
            1. **Answer strictly based on the provided process details and logs.**
            2. **Avoid speculating beyond the given logs and process data.**
            3. Don't mention "logs" or "data" - respond naturally.
            4. **If no relevant information is found, simply state: "I do not have relevant information for this query."**
            5. **Keep the response brief, clear, and professional.**
            6. Only explain what is relevant to the user's query.Do not include additional information or reasoning unless explicitly asked in the query. 
        """
        return prompt
        
    
    def generate_response(self, user_query: str) -> str:
        """
        Generates a response based on the user query, process data, and logs.

        Args:
            user_query (str): The mortgage-related user query.

        Returns:
            str: Response generated by the LLM.
        """
        process_data = self.load_htps(user_query)
        relevant_logs = self.extract_relevant_logs(process_data)
        print("Relevant Logs:",relevant_logs)
        # Construct prompt
        prompt = self.create_prompt(user_query, process_data, relevant_logs)

        # Generate response using LLM
        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        return response.choices[0].message.content.strip()



# Example usage

if __name__ == "__main__":
    log_dir="./logs"
    api_key = os.environ["OPENAI_API_KEY"]  # Replace with your API key
    responseGenerator = ProcessAwareResponseGenerator(  api_key=api_key ,log_dir=log_dir, milvus_uri="htps_vec_store.db")

    user_queries = ["Can you explain what happens in determine_disbursement_date_scenario","why does the employment tenure is classified as recently_hired?","Does my bussiness document 1120s meets the requirements??"]
    for user_query in user_queries:
        print("\nUser Query:",user_query)
        response = responseGenerator.generate_response(user_query)
        print("\nResponse:", response)
